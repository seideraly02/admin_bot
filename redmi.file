Ниже приведён пример содержимого для файла `README.md`, описывающего проект по интеграции **Kafka** и **SeaweedFS** для отправки и скачивания файлов.

---

# Проект: Kafka + SeaweedFS File Transfer

Данный проект демонстрирует, как передавать бинарные файлы через **Kafka** и сохранять их в **SeaweedFS**, а затем скачивать обратно по `fid` (File ID).

## 1. Содержание

1. [Описание](#описание)
2. [Требования](#требования)
3. [Установка](#установка)
4. [Запуск](#запуск)
   - [Запуск Producer (отправка файла)](#запуск-producer)
   - [Запуск Consumer (сохранение файла в SeaweedFS)](#запуск-consumer)
   - [Скачивание файла по fid (getFile.js)](#скачивание-файла-по-fid-getfilejs)
5. [Переменные окружения](#переменные-окружения)
6. [Возможные проблемы и решения](#возможные-проблемы-и-решения)
7. [Лицензия](#лицензия)

---

## 2. Описание

Простой пример, показывающий следующий процесс:

1. **Producer** читает локальный файл и отправляет его в **Kafka** в виде бинарных данных.
2. **Consumer** подписывается на Kafka-топик, получает файл и загружает его в **SeaweedFS**.
3. Возвращается `fid` (File ID), по которому файл можно в будущем скачать.
4. **getFile.js** позволяет скачать файл из SeaweedFS по известному `fid` и адресу Volume Server.

**Kafka** используется как транспорт для двоичных сообщений, а **SeaweedFS** как основное объектное хранилище.

---

## 3. Требования

- **Node.js** (v14 и выше)
- **npm** или **yarn**
- Запущенный **Kafka**-кластер (или хотя бы один брокер на `localhost:9092` по умолчанию)
- Запущенный **SeaweedFS**:
  - **Master Server** на `localhost:9333` (по умолчанию)
  - **Volume Server** на `:8080` (может отличаться в зависимости от конфигурации)

---

## 4. Установка

Склонируйте репозиторий и перейдите в папку проекта:

```bash
git clone <ссылка на ваш репозиторий>
cd ваш-проект
```

Установите зависимости:

```bash
npm install
```

> При необходимости укажите правильные адреса Kafka-брокеров и SeaweedFS-серверов в `config.js` либо через переменные окружения (см. ниже).

---

## 5. Запуск

### 5.1 Запуск Producer

Producer читает локальный файл (по умолчанию `data.txt`) и отправляет его в Kafka.  
По умолчанию путь к файлу настраивается в `config.js` (или в переменных окружения).

```bash
node producer.js
```
**Или** (если в `package.json` прописан скрипт):
```bash
npm run start-producer
```

При успешном запуске в консоли отобразится размер файла и сообщение об отправке в Kafka:
```
Отправляем файл размером 128 байт в Kafka...
Сообщение успешно отправлено в Kafka.
```

### 5.2 Запуск Consumer

Consumer прослушивает Kafka-топик, получает двоичные данные и загружает их в SeaweedFS.

```bash
node consumer.js
```
**Или**:
```bash
npm run start-consumer
```

После получения сообщения вы увидите логи вида:
```
Consumer запущен. Ожидаем новых сообщений...
Получен файл, размер: 128 байт
Файл загружен, fid=6,abc123
```
Где `6,abc123` — это `fid`, сохранённый SeaweedFS.  
При желании этот `fid` можно записывать в базу данных, логировать и т.д.

### 5.3 Скачивание файла по fid (`getFile.js`)

Допустим, вы узнали `fid=6,abc123` и адрес Volume Server `192.168.0.101:8080`.  
Тогда, чтобы скачать файл:

```bash
node getFile.js 6,abc123 192.168.0.101:8080
```
Если адрес Volume Server совпадает с `localhost:8080`, запустите:
```bash
node getFile.js 6,abc123 localhost:8080
```

По умолчанию файл сохранится в `downloaded_data.txt` (можно менять в `config.js` или в переменных окружения).  
Если скачивание прошло успешно, вы увидите логи:
```
Скачиваем файл: http://192.168.0.101:8080/6,abc123
Файл сохранён как ./downloaded_data.txt, размер: 128 байт
```

---

## 6. Переменные окружения

Для удобства можно настроить `.env` файл или передавать переменные окружения напрямую:

| Переменная              | Значение по умолчанию | Описание                                   |
|-------------------------|-----------------------|--------------------------------------------|
| `KAFKA_BROKERS`         | `localhost:9092`      | Адрес(а) Kafka-брокеров, через запятую     |
| `KAFKA_CLIENT_ID`       | `my-app`             | ClientId для Kafka                         |
| `KAFKA_TOPIC`           | `file_topic`         | Топик, куда отправляем/из которого читаем  |
| `KAFKA_GROUP_ID`        | `file-consumer-group`| Группа потребителей Kafka                  |
| `SEAWEED_MASTER_HOST`   | `localhost`          | Адрес Master Server SeaweedFS             |
| `SEAWEED_MASTER_PORT`   | `9333`               | Порт Master Server                         |
| `FILE_TO_SEND`          | `./data.txt`         | Локальный путь к исходному файлу          |
| `FILE_DOWNLOADED`       | `./downloaded_data.txt` | Путь для скачанного файла               |

Пример `.env`:

```env
KAFKA_BROKERS=192.168.0.50:9092
KAFKA_TOPIC=my_file_topic
SEAWEED_MASTER_HOST=192.168.0.51
SEAWEED_MASTER_PORT=9333
```

---

## 7. Возможные проблемы и решения

1. **Ошибка `ECONNREFUSED` или `Timeout`**  
   - Проверьте, что Kafka действительно запущен и доступен по порту `9092`.  
   - Убедитесь, что SeaweedFS Master запущен на `9333`, а Volume Server — на `:8080`.

2. **Файл загружается пустым (0 байт)**  
   - Убедитесь, что `producer.js` читает файл корректно (`fs.readFileSync`).  
   - Проверьте логи consumer: должен отображаться реальный размер в байтах.  

3. **`404 Not Found` или `400 Bad Request` при скачивании**  
   - Проверьте, совпадает ли `fid` (например, `6,abc123`) с тем, что возвращает Consumer.  
   - Убедитесь, что обращаетесь к **правильному** Volume Server (`lookup` в Master Server, если нужно).

4. **Сообщения повторяются в Consumer**  
   - Либо вы используете `fromBeginning: true` и Consumer заново читает старые сообщения, либо используете один и тот же Consumer Group, но сбрасываете offset.  
   - Установите `fromBeginning: false`, если нужны **только новые** сообщения.

5. **Слишком большой файл**  
   - По умолчанию Kafka имеет ограничения на размер сообщения (около 1-10 МБ). Для крупных файлов лучше передавать **только метаданные** в Kafka, а файл напрямую загружать в SeaweedFS.

---

## 8. Лицензия

Выберите подходящую лицензию (MIT, Apache 2.0 и т.п.) или оставьте раздел пустым, если не планируете публиковать исходный код в открытых источниках. Пример — **MIT**:

```
The MIT License (MIT)
Copyright ...
```

---

## Заключение

Проект демонстрирует базовые принципы передачи бинарных данных через Kafka и хранения в SeaweedFS.  
При необходимости доработайте логику для:
- Автоматического определения Volume Server через `/dir/lookup`.
- Хранения и управления списком `fid` в базе данных.
- Стриминга больших файлов.

Удачи в разработке!